## __TL;DR__
The "node" is just an Ollama wrapper with 1 (one) sample prompt that allows the pretrained, not actually finetuned model to interact with Metamask. It doesn't even enforce JSON mode or anything, so there is a non-negligible chance that the model would hallucinate midway in constructing the ETH transaction.

There is no "compute providing network", there is no router, this is not an MVP.

--------

So we've got something a little confusing going on here.

On the [main repo](https://github.com/MorpheusAIs/Morpheus), we've got installs and source code for a local model (I'll look at this later). 

We also have links to [Morpheus Local Install](https://github.com/MorpheusAIs/Node) in a repo called "Node" (since renamed to "Lite-Client") and a [Lumerin Node](https://github.com/MorpheusAIs/Morpheus-Lumerin-Node) , which is a different local program??

## __Notes__

Excerpts from the Discord FAQ:

- **Is there a platform through which I could provide compute?**  
	- None at the moment. However, work with Akash is underway to create a template to make this possible in a decentralized fashion. 
-  **Will compute providers be able to use cloud infrastructure?**  
	- Morpheus is agnostic to how Compute Providers provide Inference Per Second. Akash, Bittensor, Ritual, EdgeLlama, whatever you want.
- **How do I provide compute?**  
	- Currently, there is no way to provide compute as the compute contracts are still under development, however you can still spin up and test the local inference of the node. 
- **Is running a node the same as providing compute?**  
	- Running a node is NOT the same as providing compute, compute provision is still being finalized so hold tight for more information.

Summarising: there is no existing system to delegate compute, running a node != providing compute??, they're happy to make bold claims about how to provide compute before even having an MVP 


## __Lite Client__

Appears to have been abandoned. 


## [__The Lumerin Model__](https://github.com/MorpheusAIs/Docs/blob/main/!KEYDOCS%20README%20FIRST!/Morpheus%20Lumerin%20Model.md)

- This seems to be just another reinterpretation and reconfiguration of the routing-bidding-distributing system that's been proposed twice over already
- There isn't anything new here besides integrating the ideas of the "Techno Capital Machine" (more needless buzzwords, which in brief is just the distribution model to reward coders etc.)
- I don't know why it's a separate thing on its own but it's telling me that the project is very much confused as to what it's supposed to be.

## __The "Node" from the main repo__

Let's see how advanced this LLM actually is. Going back to the claims made in the "SmartAgents" section, it should be able to execute Web3 related tasks, provide suggestions, pick optimal contracts, have a system to develop "tasks" and is a finetuned LLM perfect for Web3 users.

I'll be trying the Windows build in a sandbox.

The experience:
- Hella slow setup time as it appears to be downloading Ollama in its entirety
	- [Ollama](https://github.com/ollama/ollama) is just an open source, readily available local LLM wrapper built by devs that are likely far more capable than those in the Morpheus team, whose work is now being piggybacked off
- After about 10 minutes, it is still Initializing
- Looking at the source code, the default model is Mistral (no finetuning whatsoever, of course). This needs a minimum of 8GB free RAM to run, so it probably wouldn't run properly... if it would even initialize.

## __Source Code__

Assuming that the source code in the repo is one and the same with the distribution, we can take a look at what's really going on here.

I'll be looking at the most interesting files as the bulk of it is boilerplate to set up a GUI for Ollama.

[agent/app.js](https://github.com/MorpheusAIs/Morpheus/blob/main/src/agent/app.js)
- This is the implementation for a SmartAgent
- Here we can see the system prompt for the model. Summarising it, it just gets the model to use a fixed JSON format and tells it to respond to the user as Morpheus.
- The advanced "memory" is just a text file, "memory.txt", where your chat log is stored. No Filecoin, no IPFS.

[agent/tools/tools.py](https://github.com/MorpheusAIs/Morpheus/blob/main/src/agent/tools/tools.py)
- PoC for tools the LLM has. 
- I get that it is a proof of concept, but could they not have done more beyond elementary math operations?

[agent/metamask-agent/backend/agents/metamask.py](https://github.com/MorpheusAIs/Morpheus/blob/main/src/agent/metamask-agent/backend/agents/metamask.py)
- Example of Web3 integration
- Provides a few sample prompts and responses for the LLM to work off. 
- Currently only capable of initiating a transaction from address A to address B

[agents/rag_assets/send_eth_transaction.txt](https://github.com/MorpheusAIs/Morpheus/blob/main/src/agent/metamask-agent/backend/agents/rag_assets/send_eth_transaction.txt)
- The advanced finetuning: a single textfile with a sample response that is pulled via RAG
- Barely even qualifies as RAG



